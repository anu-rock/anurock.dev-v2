---
title: "How LLMs Learn from the Internet: The Training Process"
url: "https://open.substack.com/pub/bytebytego/p/how-llms-learn-from-the-internet?utm_campaign=post-expanded-share&utm_medium=web"
excerpt: "In this article, we will walk through the complete journey of how LLMs are trained, from the initial collection of raw data to the final conversational assistant."
readDate: "2026-01-11T05:45:09.757Z"
---

In my 2017 book Artificial Intelligence for .NET (co-authored with Nishith), I described how language models of that era worked. A combination of gradient descent and supervised fine-tuning gave them predictive and classification superpowers for the given domain/context. Those models were next token predictors in the truest sense.

The attention/transformers architecture together with data-science-optimized GPUs  revolutionized language models and made today's LLMs possible. The fundamentals of pre-training and teaching models have stayed the same.

It's incredible how a single breakthrough in software architecture can lead to generational advancements.

Beautifully written article by Alex Xu, so easy to digest. Highly recommended even if you aren't familiar with the technical details.