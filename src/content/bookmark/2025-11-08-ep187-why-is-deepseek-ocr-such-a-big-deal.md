---
title: "EP187: Why is DeepSeek-OCR such a BIG DEAL?"
url: "https://blog.bytebytego.com/p/ep187-10-key-data-structures-we-use?utm_campaign=post&utm_medium=web"
excerpt: "Over to you: Which additional data structures have we overlooked?"
readDate: "2025-11-08T04:50:36.959Z"
---

Let me guess: companies like OpenAI, Claude, and Google won't be keen on implementing this approach as they make their millions ($$) from token costs. Anyhow, converting text into *compressed* visual tokens seems like a clever way to significantly reduce number of tokens that reach LLM's decoder layer. I hope DeepSeek's OCR is incredibly accurate, unlike traditional OCR models.
