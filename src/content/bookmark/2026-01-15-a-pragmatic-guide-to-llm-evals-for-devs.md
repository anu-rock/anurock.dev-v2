---
title: "A pragmatic guide to LLM evals for devs"
url: "https://open.substack.com/pub/pragmaticengineer/p/evals?utm_campaign=post-expanded-share&utm_medium=web"
excerpt: "Evals are a new toolset for any and all AI engineers â€“ and software engineers should also know about them. Move from guesswork to a systematic engineering process for improving AI quality."
readDate: "2026-01-15T03:43:49.124Z"
---

TIL about Evals, the automated testing analogue to traditional unit/integration tests. Since running LLMs (for evaluation) in CI pipelines isn't cheap, it's good to prioritize test scenarios based on top buckets of real-world user issues.
